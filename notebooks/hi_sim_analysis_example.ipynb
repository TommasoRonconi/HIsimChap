{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7944d3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6cc3d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "def load_from_hdf5 ( infile ) :\n",
    "    import os\n",
    "\n",
    "    # check if file exists\n",
    "    if not os.path.isfile( infile ) :\n",
    "        raise IOError( f\"file {infile} does not exist.\" )\n",
    "        \n",
    "    indict = {}\n",
    "    with h5py.File(infile, 'r') as f :\n",
    "        indict['metadata'] = dict(f.attrs)\n",
    "        for name, todict in f.items() :\n",
    "            try :\n",
    "                indict[name] = { k : todict.get(k)[:] for k in todict.keys() }\n",
    "            except Exception as err :\n",
    "                warnings.warn(\n",
    "                    f\"- Skipping group '{name}' as it raises \"\n",
    "                    f\"exception {type(err).__name__} with message:\\n  '{err}'\"\n",
    "                )\n",
    "            kmeta = list(todict.attrs.keys())\n",
    "            vmeta = list(todict.attrs.values())\n",
    "            if len(kmeta)>0 and len(vmeta>0) :\n",
    "                indict['metadata'] = dict(zip(kmeta,vmeta))\n",
    "        return indict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aadb4868",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalogues = 'catalogue_hi_slice{0:0>2d}.hdf5'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732ece32",
   "metadata": {},
   "source": [
    "# Statistics: 1-point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da475c6a",
   "metadata": {},
   "source": [
    "## redshift distribution (IM: z<3, Gxys: z<0.5)\n",
    "\n",
    "Computed as\n",
    "\n",
    "$$\\mathcal{N}(z) = \\dfrac{1}{N_\\text{tot}}\\cdot\\dfrac{\\text{d}N(z)}{\\text{d}z}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58a7edd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_bins = numpy.linspace(0,3,25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f37b3c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "zdist = numpy.zeros(z_bins.size-1, dtype=float)\n",
    "for ii in range(3) :\n",
    "    data = load_from_hdf5(catalogues.format(ii))\n",
    "    red = data['galaxies']['Red']\n",
    "    zdist += numpy.histogram( red, bins=z_bins )[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c90254",
   "metadata": {},
   "source": [
    "## Omega_HI\n",
    "\n",
    "Defined as\n",
    "\n",
    "$$\\Omega_\\text{HI}(z) = \\dfrac{\\rho_\\text{HI}(z)}{\\rho_\\text{crit}(z=0)}\\approx \\dfrac{\\text{d}M_\\text{HI}^\\text{tot}(z)}{\\text{d}V(z)}\\cdot\\dfrac{1}{\\rho_\\text{crit}(z=0) }$$\n",
    "\n",
    "We want \n",
    "* the binned total mass \n",
    "* volume of the redshift slice (in your cosmology)\n",
    "* critical density (in your cosmology)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36ac13a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_bins = numpy.linspace(0,3,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0ede29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "zmhi = numpy.zeros(z_bins.size-1, dtype=float)\n",
    "for ii in range(3) :\n",
    "    data = load_from_hdf5(catalogues.format(ii))\n",
    "    \n",
    "    red = data['galaxies']['Red']\n",
    "    mhi = data['galaxies']['Mhi']\n",
    "    dig = numpy.digitize(red, z_bins)-1\n",
    "    numpy.add.at( zmhi, dig, mhi )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b307abd6",
   "metadata": {},
   "source": [
    "## MHI-Mh relation\n",
    "\n",
    "We want\n",
    "* the histograms with the total counts in the two dimensions ($M_\\text{halo}$, $M_\\text{HI}$) \n",
    "* divided in redshift bins?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46afe809",
   "metadata": {},
   "outputs": [],
   "source": [
    "MDM_bins = numpy.logspace(7,15,1001)\n",
    "MHI_bins = numpy.logspace(1,12,1001)\n",
    "z_bins = numpy.linspace(0,3,7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5809ddb7",
   "metadata": {},
   "source": [
    "> **without** redshift binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97f2bfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no redshift bins\n",
    "MHIMh = numpy.zeros((MDM_bins.size-1,MHI_bins.size-1), dtype=float)\n",
    "for ii in range(3) :\n",
    "    data = load_from_hdf5(catalogues.format(ii))\n",
    "    \n",
    "    # Extract relevant data\n",
    "    mdm = data['galaxies']['Mdm']   # Halo mass\n",
    "    mhi = data['galaxies']['Mhi'] # HI mass\n",
    "    \n",
    "    # Sum counts\n",
    "    MHIMh += numpy.histogram2d(mdm, mhi, bins=(MDM_bins, MHI_bins))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb31951",
   "metadata": {},
   "source": [
    "> **with** redshift binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eae17d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with redshift bins\n",
    "MHIMh = numpy.zeros((z_bins.size-1,MDM_bins.size-1,MHI_bins.size-1), dtype=float)\n",
    "for ii in range(3) :\n",
    "    data = load_from_hdf5(catalogues.format(ii))    \n",
    "    \n",
    "    # Extract relevant data\n",
    "    red = data['galaxies']['Red']     # Redshift\n",
    "    mdm = data['galaxies']['Mdm']     # Halo mass\n",
    "    mhi = data['galaxies']['Mhi'] # HI mass\n",
    "    \n",
    "    # Compute bin indices\n",
    "    z_dig = numpy.digitize(red, z_bins) - 1 \n",
    "    mdm_dig = numpy.digitize(mdm, MDM_bins) - 1\n",
    "    mhi_dig = numpy.digitize(mhi, MHI_bins) - 1\n",
    "\n",
    "    # Filter valid indices to avoid out-of-bound errors\n",
    "    valid = (\n",
    "        (0 <= z_dig) & (z_dig < z_bins.size - 1) &\n",
    "        (0 <= mdm_dig) & (mdm_dig < MDM_bins.size - 1) &\n",
    "        (0 <= mhi_dig) & (mhi_dig < MHI_bins.size - 1)\n",
    "    )\n",
    "    \n",
    "    # Sum counts\n",
    "    numpy.add.at(MHIMh, (z_dig[valid], mdm_dig[valid], mhi_dig[valid]), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a748c1e",
   "metadata": {},
   "source": [
    "## HI mass function\n",
    "\n",
    "We want \n",
    "* the mass binned counts \n",
    "* volume of the redshift slice (in your cosmology)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e01ef9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MHI_bins = numpy.logspace(1,12,51)\n",
    "z_bins = numpy.linspace(0,0.5,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57ed8a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "NMHI = numpy.zeros((z_bins.size-1,MHI_bins.size-1), dtype=float)\n",
    "for ii in range(3) :\n",
    "    data = load_from_hdf5(catalogues.format(ii)) \n",
    "    \n",
    "    # Extract relevant data\n",
    "    red = data['galaxies']['Red']     # Redshift\n",
    "    mhi = data['galaxies']['Mhi'] # HI mass\n",
    "    \n",
    "    # Compute bin indices\n",
    "    z_dig = numpy.digitize(red, z_bins) - 1 \n",
    "    mhi_dig = numpy.digitize(mhi, MHI_bins) - 1\n",
    "\n",
    "    # Filter valid indices to avoid out-of-bound errors\n",
    "    valid = (\n",
    "        (0 <= z_dig) & (z_dig < z_bins.size - 1) &\n",
    "        (0 <= mhi_dig) & (mhi_dig < MHI_bins.size - 1)\n",
    "    )\n",
    "    \n",
    "    # Sum counts\n",
    "    numpy.add.at(NMHI, (z_dig[valid], mhi_dig[valid]), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3c3476-d079-4acc-80ac-be84fc506ba8",
   "metadata": {},
   "source": [
    "# 2-point\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760bf0e2",
   "metadata": {},
   "source": [
    "## Power spectrum on a box\n",
    "\n",
    "Paco's python library + tutorial (it's pip-installable):\n",
    "* [Density field tutorial](https://pylians3.readthedocs.io/en/master/construction.html)\n",
    "* [Pk tutorial](https://pylians3.readthedocs.io/en/master/Pk.html)\n",
    "\n",
    "Minimal example usage:\n",
    "```python\n",
    "import numpy\n",
    "import Pk_library as PKL\n",
    "\n",
    "Npix = 2**8\n",
    "boxside = 512.0 # cMpc/h\n",
    "pixsize = boxside/Npix\n",
    "Npart = 1024**3 # just as an example\n",
    "\n",
    "# density contrast (uniform for testing)\n",
    "delta = numpy.random.default_rng().uniform(\n",
    "    size=(Npix,Npix,Npix)\n",
    ").astype(numpy.float32)\n",
    "\n",
    "# allocate power spectrum object (here the computation happens)\n",
    "Pk = PKL.Pk(\n",
    "    delta,        # the density contrast\n",
    "    boxside,      # to get the scale\n",
    "    axis=0,  \n",
    "    MAS='NGP',    # different possible mass assignment choices, here NGP=Nearest Grid Point  \n",
    "    threads=8,    # parallel threads (shared-memory parallel)\n",
    "    verbose=True \n",
    ")\n",
    "\n",
    "# extract the 3D P(k) and corresponding k grid (no computation here)\n",
    "kh  = Pk.k3D\n",
    "Pk0 = Pk.Pk[:,0] #monopole\n",
    "\n",
    "# limits \n",
    "shotnoise = boxside**3/Npart # depends on the density of objects \n",
    "kNyq = 2*numpy.pi / pixsize  # Nyquist frequency\n",
    "kMin = 2*numpy.pi / boxside  # Maximum scale for the given box\n",
    "```\n",
    "```\n",
    "> Computing power spectrum of the field...\n",
    "> Time to complete loop = 0.70\n",
    "> Time taken = 0.76 seconds\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc26eef",
   "metadata": {},
   "source": [
    "## Cls projected map\n",
    "\n",
    "Compute at will, provide \n",
    "* the $\\mathcal{l}$ \n",
    "* corresponding $\\mathcal{C}(\\mathcal{l})$ \n",
    "* the $f_\\text{sky}$\n",
    "* the shot noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe78d6e",
   "metadata": {},
   "source": [
    "## bias?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3b7c20",
   "metadata": {},
   "source": [
    "# Output format\n",
    "\n",
    "To ease the comparison process, let's stick to the same file format. ``hdf5`` is portable and compact and includes natively metadata.\n",
    "Whatever the method used to compute the statistics above it shouldn't be too difficult to pack them into a standardized ``hdf5``.\n",
    "\n",
    "The quantities to store are:\n",
    "* the redshift distribution ``zdist``\n",
    "* the density parameter of hydrogen (in terms of total hydrogen mass at varying redshift ``zmhi`` and volume per redshift bin ``Volume``)\n",
    "* the $M_\\text{HI}$ to $M_\\text{halo}$ mass ratio (in terms of 2D hystograms with number counts per bin, ``MHIMh``)\n",
    "* the HI mass function (in terms of number of objects of given mass per bin ``NMHI`` and volume per redshift bin ``Volume``)\n",
    "\n",
    "> For all of the above it will not be necessary to store the used bins as we are aiming to have everything computed on the same grid.\n",
    "> We are nevertheless asking for all those quantities that directly depend on your cosmological parameters (e.g. the **comoving volume in h-units** of the redshift bin) \n",
    "\n",
    "2 point statistics instead will be re-binned a posteriori, you can give us\n",
    "* the power spectrum and related quantities:\n",
    "    - ``kh``\n",
    "    - ``Pk0``\n",
    "    - ``shotnoise``\n",
    "    - ``kNyq``\n",
    "    - ``kMin``\n",
    "* the Cells and related quantities:\n",
    "    - ``Cell``\n",
    "    - ``ell``\n",
    "    - ``shot``\n",
    "    - ``fsky``\n",
    "\n",
    "In python you would do something like this\n",
    "\n",
    "```python\n",
    "import h5py\n",
    "\n",
    "outfile = '/path/to/measurements/YourName.hdf5'\n",
    "with h5py.File( outfile, \"w\" ) as f :\n",
    "            \n",
    "    # store eventual meta-data\n",
    "    #f.attrs[''] = \n",
    "\n",
    "    ########################################################\n",
    "    # store groups:\n",
    "    \n",
    "    # Redshift distribution\n",
    "    group_zdist = f.create_group('redshift_distribution')\n",
    "    group_zdist.create_dataset( 'zdist', data = zdist )\n",
    "    \n",
    "    # Density parameter\n",
    "    group_omega = f.create_group('omega_hi')\n",
    "    group_omega.create_dataset( 'zmhi', data = zmhi )\n",
    "    group_omega.create_dataset( 'Vz', data = vslice )\n",
    "    \n",
    "    # MHIMh relation\n",
    "    group_mhimh = f.create_group('MHIMh')\n",
    "    group_mhimh.create_dataset('NMHI-NMh', data = MHIMh )\n",
    "    \n",
    "    # HI mass function\n",
    "    group_nmhi = f.create_group('nmhi')\n",
    "    group_nmhi.create_dataset('NMHI', data = NMHI )\n",
    "    group_nmhi.create_dataset('Vz', data = vslice)\n",
    "    \n",
    "    # power spectrum (on a box)\n",
    "    group_pk = f.create_group('pk')\n",
    "    group_pk.create_dataset('kh', data = kh)\n",
    "    group_pk.create_dataset('Pk0', data = Pk0)\n",
    "    group_pk.attrs['shot'] = shotnoise\n",
    "    group_pk.attrs['kMin'] = kMin \n",
    "    group_pk.attrs['kNyq'] = kNyq\n",
    "    \n",
    "    # power spectrum (on a sphere/lightcone)\n",
    "    group_pk = f.create_group('cell')\n",
    "    group_pk.create_dataset('ell', data = ell)\n",
    "    group_pk.create_dataset('Cell', data = Cell)\n",
    "    group_pk.attrs['shot'] = shotnoise\n",
    "    group_pk.attrs['fsky'] = fsky\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be525fa",
   "metadata": {},
   "source": [
    "## An example reading routine (for completeness)\n",
    "\n",
    "The cell below shows how it will be possible to read into a python dictionary all the content of a file formatted as above.\n",
    "\n",
    "Given that you have stored the dataset in a file ``outfile = '/path/to/dataset.h5'``\n",
    "```python\n",
    "indict = {}\n",
    "with h5py.File(outfile, 'r') as f :\n",
    "     for name, todict in f.items() :\n",
    "            try :\n",
    "                indict[name] = { k : todict.get(k)[:] for k in todict.keys() }\n",
    "            except Exception as err :\n",
    "                warnings.warn(\n",
    "                    f\"- Skipping group '{name}' as it raises \"\n",
    "                    f\"exception {type(err).__name__} with message:\\n  '{err}'\"\n",
    "                )\n",
    "            kmeta = list(todict.attrs.keys())\n",
    "            vmeta = list(todict.attrs.values())\n",
    "            if len(kmeta)>0 and len(vmeta>0) :\n",
    "                indict['metadata'] = dict(zip(kmeta,vmeta))\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
